# textgenerator
Text generator - Bigram model (Markov chain)

Схема работы:
1) Разбиваем исходный текст на последовательность токенов (слов, знаков препинания);
2) Составляем биграммы. Например, для предложения "Москва - столица России." будут сгенерированы следующие биграммы:
{"#", "Москва" }
{"Москва", "-" }
{"-", "столица"}
{"столица", "России"}
{"России", "."}
{".", "#"}
3) Для каждого токена составляем список токенов, идущих после него, и вычисляем вероятность по данной формуле:
P("России" | "столица") = COUNT("столица России") / COUNT("столица")
4) Генерируем предложение исходя из этих вероятностей.
